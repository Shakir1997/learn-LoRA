{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9LdG6HYWY9d0erlg5SIGa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Singular Value Decomposition**"],"metadata":{"id":"t1GwAETmKe4I"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5UOH-KQxKnCD","executionInfo":{"status":"ok","timestamp":1729256732447,"user_tz":-330,"elapsed":3876,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"7b2df4a1-cc93-4528-caa9-e1e6bf0730f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7cf5e47a4ff0>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["## Generate Rank deficient Matrix"],"metadata":{"id":"pUZO9C3jRzZp"}},{"cell_type":"code","source":["import torch\n","\n","def generate_rank_deficient_matrix(shape, rank):\n","    \"\"\"\n","    Generates a rank-deficient matrix of a given shape and rank.\n","\n","    Args:\n","        shape (tuple): Shape of the matrix as (rows, cols).\n","        rank (int): Desired rank of the matrix.\n","\n","    Returns:\n","        torch.Tensor: A rank-deficient matrix with the specified rank.\n","    \"\"\"\n","    rows, cols = shape\n","\n","    # Ensure rank is valid (it can't be higher than the minimum of rows or columns)\n","    if rank > min(rows, cols):\n","        raise ValueError(\"Rank cannot be greater than the minimum of the number of rows or columns.\")\n","\n","    # Step 1: Create a random matrix of the specified rank\n","    # Create two random matrices and multiply them to create a matrix of the desired rank\n","    A = torch.randn(rows, rank)  # (rows, rank)\n","    B = torch.randn(rank, cols)  # (rank, cols)\n","\n","    # Step 2: Multiply them to get a matrix of shape (rows, cols) with rank `rank`\n","    rank_deficient_matrix = A @ B\n","\n","    # Check the rank of the generated matrix\n","    generated_rank = torch.linalg.matrix_rank(rank_deficient_matrix)\n","\n","    assert generated_rank == rank, f\"Generated matrix does not have the desired rank: {generated_rank} != {rank}\"\n","\n","    return rank_deficient_matrix, generated_rank"],"metadata":{"id":"LJDmu9ikOK5e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example usage\n","m, n = 20, 20\n","rank = 2\n","W, W_rank = generate_rank_deficient_matrix((m, n), rank)\n","print(\"Rank of the Matrix:\", W_rank.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R15I3FHVPvQS","executionInfo":{"status":"ok","timestamp":1729256732448,"user_tz":-330,"elapsed":9,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"8cde4e67-f80a-459d-b7a3-6870b635f5fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Rank of the Matrix: 2\n"]}]},{"cell_type":"markdown","source":["## Apply SVD on generated matrix (W)"],"metadata":{"id":"-G7vyv9SSKIU"}},{"cell_type":"code","source":["# Perform SVD on W (W = U x D x V^T)\n","U, D, V = torch.svd(W)\n","\n","# Select only W_rank singular values & vectors\n","U_r = U[:, :W_rank]\n","D_r = D[:W_rank]\n","V_r = V[:, :W_rank].t()\n","\n","# Compute B = U_r * D_r and A = V_r\n","A = V_r\n","B = U_r * D_r\n","print(f'Shape of A: {A.shape}')\n","print(f'Shape of B: {B.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r338N9GcSMdR","executionInfo":{"status":"ok","timestamp":1729256732448,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"73cd820f-93c9-47f2-8ed7-37d0ebe2f463"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of A: torch.Size([2, 20])\n","Shape of B: torch.Size([20, 2])\n"]}]},{"cell_type":"markdown","source":["## Check the Difference between output"],"metadata":{"id":"4l33mJC7gd4C"}},{"cell_type":"code","source":["# Generate random input and bias\n","x = torch.randn(m)\n","b = torch.randn(m)\n","\n","# Compute y = Wx + b\n","y = W @ x + b\n","\n","# Compute y' = (B @ A) @ x + b\n","y_r = (B @ A) @ x + b\n","\n","print(\"Original y using W:\\n\", y)\n","print(\"\")\n","print(\"y' computed using BA:\\n\", y_r)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzVJwNHrTNSM","executionInfo":{"status":"ok","timestamp":1729256732449,"user_tz":-330,"elapsed":7,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"4ebbb741-6a48-483b-915d-c808c05cf06a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original y using W:\n"," tensor([ 7.6824, -3.4827, -2.3524, -3.8416,  3.4325, -4.4672, -2.2232,  1.4218,\n","         1.5892,  0.1178,  0.4996,  5.2467,  1.8231,  0.4579,  3.7740, -2.8144,\n","        -1.8249,  4.5885,  0.9263, -5.0501])\n","\n","y' computed using BA:\n"," tensor([ 7.6824, -3.4827, -2.3524, -3.8416,  3.4325, -4.4672, -2.2232,  1.4218,\n","         1.5892,  0.1178,  0.4996,  5.2467,  1.8231,  0.4579,  3.7740, -2.8144,\n","        -1.8249,  4.5885,  0.9263, -5.0501])\n"]}]},{"cell_type":"code","source":["print(\"Total parameters of W: \", W.nelement())\n","print(\"Total parameters of B and A: \", B.nelement() + A.nelement())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekksTMgxhhqw","executionInfo":{"status":"ok","timestamp":1729256732449,"user_tz":-330,"elapsed":6,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"4509d22d-004a-4d8e-e08e-52828e84a0d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters of W:  400\n","Total parameters of B and A:  80\n"]}]},{"cell_type":"markdown","source":["# LORA Implementation"],"metadata":{"id":"yGGWWSEIogx8"}},{"cell_type":"markdown","source":["*https://github.com/pytorch/examples/blob/main/mnist/main.py*"],"metadata":{"id":"8qkQOtgdlen-"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6DQVZBLwomol","executionInfo":{"status":"ok","timestamp":1729256734600,"user_tz":-330,"elapsed":2155,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"22d9bf77-f84d-42ed-f637-23c7e2949192"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7cf5e47a4ff0>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["## Load Dataset"],"metadata":{"id":"1g2ZJAOu6lA2"}},{"cell_type":"code","source":["transform=transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","        ])\n","\n","train_kwargs = {'batch_size': 512, 'shuffle': True}\n","test_kwargs = {'batch_size': 512, 'shuffle': True}\n","\n","# Load MNIST train and test dataset\n","dataset1 = datasets.MNIST('../data', train=True, download=True, transform=transform)\n","dataset2 = datasets.MNIST('../data', train=False, transform=transform)\n","\n","# Create dataloader for training and testing\n","train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n","test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"],"metadata":{"collapsed":true,"id":"HCozwma0pFji"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"jrZFik03rcjf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Create Neural Network"],"metadata":{"id":"WLsh8PTT6sQ3"}},{"cell_type":"code","source":["# Create neural network to classifiy the MNIST dataset, make it with more parameters to represent lora much better\n","\n","class ComplicatedNetwork(nn.Module):\n","    def __init__(self, hidden_layer_1=1000, hidden_layer_2=2000):\n","        super(ComplicatedNetwork, self).__init__()\n","        self.fc1 = nn.Linear(28*28, hidden_layer_1)\n","        self.fc2 = nn.Linear(hidden_layer_1, hidden_layer_2)\n","        self.fc3 = nn.Linear(hidden_layer_2, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","      x = x.view(-1, 28*28)\n","      x = self.relu(self.fc1(x))\n","      x = self.relu(self.fc2(x))\n","      x = self.fc3(x)\n","      return x\n","\n","cnet = ComplicatedNetwork().to(device)"],"metadata":{"id":"LnM_kKJ2rs8G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train"],"metadata":{"id":"aOVtacaW6xJg"}},{"cell_type":"code","source":["def train(train_loader, model, epochs=10, total_iterations_limit=None):\n","  loss = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","  total_iterations = 0\n","  for epoch in range(epochs):\n","    model.train()\n","\n","    loss_sum = 0\n","    num_iterations = 0\n","\n","    data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n","\n","    if total_iterations_limit is not None:\n","      data_iterator.total = total_iterations_limit\n","\n","    for data, target in data_iterator:\n","      num_iterations += 1\n","      total_iterations += 1\n","\n","      data, target = data.to(device), target.to(device)\n","\n","      optimizer.zero_grad()\n","      output = model(data.view(-1, 28*28))\n","      loss_value = loss(output, target)\n","      loss_sum += loss_value.item()\n","\n","      avg_loss = loss_sum / num_iterations\n","      data_iterator.set_postfix(loss=avg_loss)\n","\n","      loss_value.backward()\n","      optimizer.step()\n","\n","      if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n","        return\n"],"metadata":{"id":"-_MwHF2avfU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(train_loader, cnet, epochs=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6FUuppcwPrt","executionInfo":{"status":"ok","timestamp":1729256775419,"user_tz":-330,"elapsed":40420,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"387201a4-250b-42aa-e263-e91637af5101"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 118/118 [00:40<00:00,  2.90it/s, loss=0.258]\n"]}]},{"cell_type":"code","source":["original_weights = {}\n","for name, param in cnet.named_parameters():\n","    original_weights[name] = param.clone().detach()"],"metadata":{"id":"ExEV55VkgogC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing"],"metadata":{"id":"yzg_mngT61CF"}},{"cell_type":"code","source":["def infer():\n","  correct = 0\n","  total = 0\n","\n","  wrong_count = [0 for i in range(10)]\n","\n","  with torch.no_grad():\n","    for data, target in test_loader:\n","      data, target = data.to(device), target.to(device)\n","      output = cnet(data.view(-1, 28*28))\n","\n","      for idx, i in enumerate(output):\n","        if torch.argmax(i) == target[idx]:\n","          correct += 1\n","        else:\n","          wrong_count[target[idx]] += 1\n","        total += 1\n","    print(f'Accuracy: {round(correct/total, 3)}')\n","    for i in range(len(wrong_count)):\n","      print(f'Wrong predictions for class {i}: {wrong_count[i]}')\n","\n","infer()"],"metadata":{"id":"dncZiBYtwvvA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1729256780011,"user_tz":-330,"elapsed":4596,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"772f514d-f26a-4785-cca1-4abf8d29dd54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.965\n","Wrong predictions for class 0: 17\n","Wrong predictions for class 1: 16\n","Wrong predictions for class 2: 30\n","Wrong predictions for class 3: 32\n","Wrong predictions for class 4: 15\n","Wrong predictions for class 5: 35\n","Wrong predictions for class 6: 29\n","Wrong predictions for class 7: 40\n","Wrong predictions for class 8: 39\n","Wrong predictions for class 9: 97\n"]}]},{"cell_type":"markdown","source":["## Visualize total parameters in network"],"metadata":{"id":"-SsQRo6g65Nh"}},{"cell_type":"code","source":["# total_parameters_original = sum(p.numel() for p in cnet.parameters())\n","# total_parameters_original"],"metadata":{"id":"K-l6DgpV6_eL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_parameters_original = 0\n","\n","for index, layer in enumerate([cnet.fc1, cnet.fc2, cnet.fc3]):\n","  total_parameters_original += layer.weight.nelement() + layer.bias.nelement()\n","  print(f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape}')\n","\n","print(f'Total number of parameters: {total_parameters_original:,}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMLD2iQL7IVJ","executionInfo":{"status":"ok","timestamp":1729256780012,"user_tz":-330,"elapsed":17,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"bbf52bb1-d3d2-47aa-ec52-bf0fd4dd748e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000])\n","Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000])\n","Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10])\n","Total number of parameters: 2,807,010\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ki8ghTz574Vd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LoRA Parameters"],"metadata":{"id":"FAO8Wj6NBIgD"}},{"cell_type":"code","source":["class LoRANetwork(nn.Module):\n","  def __init__(self, feature_in, feature_out, rank=1, alpha=1, device='cpu'):\n","    super().__init__()\n","    self.lora_A = nn.Parameter(torch.zeros(rank, feature_out)).to(device)\n","    self.lora_B = nn.Parameter(torch.zeros(feature_in, rank)).to(device)\n","    nn.init.normal_(self.lora_A, mean=0, std=1)\n","\n","    self.scale = alpha / rank\n","    self.enabled = True\n","\n","  def forward(self, original_weights):\n","    if self.enabled:\n","      return original_weights + (self.lora_B @ self.lora_A).view(original_weights.shape) * self.scale\n","    else:\n","      return original_weights\n"],"metadata":{"id":"AmuPcacmBLwZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The line `import torch.nn.utils.parametrize as parametrize` is importing the `parametrize` module from `torch.nn.utils` in PyTorch. The `parametrize` module is a feature that allows you to apply constraints, transformations, or reparameterizations to `torch.nn.Module` parameters.\n","\n","### What Does `parametrize` Do?\n","\n","In PyTorch, `parametrize` provides a way to register and manage \"parameterizations\" of a module's parameters. A parameterization is a transformation applied to a parameter of a neural network layer before it is used in computations. For example, if you want to constrain a weight matrix to be symmetric or positive definite, you can use `parametrize` to enforce this constraint.\n","\n","### Common Use Cases\n","\n","1. **Enforcing Constraints:** You can use parameterizations to enforce certain constraints, like making a parameter non-negative or symmetric.\n","2. **Custom Transformations:** It allows you to apply custom transformations to parameters, such as normalizing or projecting them into a specific space.\n","3. **Reparameterization Tricks:** Useful in variational inference or other cases where you need to reparameterize your model for optimization.\n","\n","### How Does It Work?\n","\n","1. **Register a Parameterization:**\n","   - You can register a parameterization for a parameter in a `torch.nn.Module`. The parameterization is applied every time the parameter is accessed.\n","2. **Parameterized Attribute:**\n","   - The original parameter is replaced with a new attribute, and the parameterized version is computed using the registered transformation.\n","\n","### Example\n","\n","Here is a basic example of how you might use `parametrize` to enforce a weight matrix in a linear layer to be non-negative:\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.nn.utils.parametrize as parametrize\n","\n","# Define a custom parameterization\n","class PositiveParametrize(torch.nn.Module):\n","    def forward(self, X):\n","        return torch.abs(X)\n","\n","# Create a simple linear layer\n","linear = nn.Linear(10, 5)\n","\n","# Apply the parameterization to the weight parameter of the linear layer\n","parametrize.register_parametrization(linear, \"weight\", PositiveParametrize())\n","\n","# Now, linear.weight will always be non-negative\n","print(linear.weight)\n","```\n","\n","In this example:\n","- We define a custom parameterization that ensures the weights are non-negative by applying the `torch.abs()` function.\n","- We register this parameterization with the `linear` layer, so the weight matrix is always transformed to be non-negative whenever it is accessed.\n","\n","### Summary\n","\n","`parametrize` in PyTorch provides a flexible way to apply constraints or transformations to module parameters. It is particularly useful for adding custom behavior to parameters while keeping the code clean and modular."],"metadata":{"id":"K8f-eTmgowqg"}},{"cell_type":"markdown","source":["The code you provided is applying a custom parameterization to the weights of several linear layers in a neural network using the `parametrize` module in PyTorch. The goal is to replace the original weights with a parameterized version, likely based on a Low-Rank Adaptation (LoRA) approach.\n","\n","### Breakdown of the Code\n","\n","1. **Import the Parametrize Module:**\n","   ```python\n","   import torch.nn.utils.parametrize as parametrize\n","   ```\n","   This line imports the `parametrize` module, which allows you to register parameterizations for a neural network's parameters.\n","\n","2. **Define a Function for Linear Layer Parameterization:**\n","   ```python\n","   def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n","       feature_in, feature_out = layer.weight.shape\n","       lora_layer = LoRANetwork(feature_in, feature_out, rank, lora_alpha, device)\n","       return lora_layer\n","   ```\n","   - The function `linear_layer_parameterization` takes in a layer (e.g., a linear layer), a device (CPU or GPU), and optional parameters `rank` and `lora_alpha`.\n","   - It extracts the input and output feature dimensions from the weight shape of the given layer.\n","   - It then creates a `LoRANetwork` object, which is likely a custom class you have defined for applying Low-Rank Adaptation to the weights, using the specified rank and alpha values.\n","   - This function returns the `LoRANetwork` object that will be used to replace the original weights with a parameterized version.\n","\n","3. **Register Parameterizations for Different Layers:**\n","   ```python\n","   parametrize.register_parametrization(cnet.fc1,\n","         \"weight\", linear_layer_parameterization(cnet.fc1, device))\n","   parametrize.register_parametrization(cnet.fc2,\n","         \"weight\", linear_layer_parameterization(cnet.fc2, device))\n","   parametrize.register_parametrization(cnet.fc3,\n","         \"weight\", linear_layer_parameterization(cnet.fc3, device))\n","   ```\n","   - These lines use the `parametrize.register_parametrization` method to replace the weights of layers `fc1`, `fc2`, and `fc3` of the neural network `cnet`.\n","   - The parameterization is applied to the \"weight\" parameter of each layer, using the `linear_layer_parameterization` function.\n","   - This effectively reparameterizes the original weight with the `LoRANetwork` transformation, allowing for efficient low-rank updates.\n","\n","### What is LoRA?\n","\n","Low-Rank Adaptation (LoRA) is a technique used to adapt the weights of pre-trained models with a low-rank approximation, which reduces the number of parameters that need to be updated. By adding a low-rank matrix to the original weight, it makes fine-tuning more efficient, especially in large models.\n","\n","### Explanation of the Workflow\n","\n","1. **Original weights are replaced with the low-rank approximation** using the `LoRANetwork` parameterization.\n","2. **The `parametrize` module manages the parameterization**, allowing you to apply transformations while keeping the original model structure.\n","3. **The registered parameterizations ensure the modified weights are used** whenever the layers are involved in forward passes.\n","\n","This approach can help fine-tune models with fewer parameters while preserving the structure and efficiency of the original model."],"metadata":{"id":"b5_GWLYmppQ_"}},{"cell_type":"code","source":["import torch.nn.utils.parametrize as parametrize\n","\n","def linear_layer_parameterization(layer, device, rank=1, lora_alpha=1):\n","\n","  feature_in, feature_out = layer.weight.shape\n","  lora_layer = LoRANetwork(feature_in, feature_out, rank, lora_alpha, device)\n","\n","  return lora_layer\n","\n","parametrize.register_parametrization(cnet.fc1,\n","      \"weight\", linear_layer_parameterization(cnet.fc1, device))\n","parametrize.register_parametrization(cnet.fc2,\n","      \"weight\", linear_layer_parameterization(cnet.fc2, device))\n","parametrize.register_parametrization(cnet.fc3,\n","      \"weight\", linear_layer_parameterization(cnet.fc3, device))\n","\n","def enable_disable_lora(enabled=True):\n","    for layer in [cnet.fc1, cnet.fc2, cnet.fc3]:\n","        layer.parametrizations[\"weight\"][0].enabled = enabled"],"metadata":{"id":"w4vZBOzTC1L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The function `enable_disable_lora` allows for toggling the use of the LoRA (Low-Rank Adaptation) parameterization on or off for the weights of the specified layers in the neural network (`cnet`). Here's how it works in detail:\n","\n","### How the Function Works\n","\n","1. **Definition of the Function:**\n","   ```python\n","   def enable_disable_lora(enabled=True):\n","       for layer in [cnet.linear1, cnet.linear2, cnet.linear3]:\n","           layer.parametrizations[\"weight\"][0].enabled = enabled\n","   ```\n","   - The function takes a single parameter, `enabled`, which defaults to `True`. This parameter controls whether the LoRA parameterization should be active or not.\n","   - The function iterates over a list of layers `[cnet.linear1, cnet.linear2, cnet.linear3]`, assuming these are linear layers in the neural network `cnet`.\n","\n","2. **Accessing the Parameterization:**\n","   - For each layer, the function accesses the `parametrizations` attribute for the `\"weight\"` parameter. This attribute is a list, and the first (and only) element is the `LoRANetwork` object.\n","   - The function then sets the `enabled` attribute of the `LoRANetwork` object to the value of the `enabled` parameter passed to `enable_disable_lora`.\n","\n","3. **How It Controls the LoRA Behavior:**\n","   - The `LoRANetwork` class has an `enabled` attribute that determines whether the parameterization is applied in the `forward` method.\n","   - If `self.enabled` is `True`, the LoRA adaptation `(self.lora_B @ self.lora_A).view(original_weights.shape) * self.scale` is added to the original weights.\n","   - If `self.enabled` is `False`, the original weights are returned without any modifications.\n","\n","### What Happens When You Call the Function\n","\n","- **`enable_disable_lora(True)`**: Enables the LoRA adaptation for the specified layers, so the weights will be adjusted with the low-rank approximation during forward passes.\n","- **`enable_disable_lora(False)`**: Disables the LoRA adaptation, making the network use the original weights directly without applying the low-rank modification.\n","\n","### Use Case\n","\n","This function is useful for turning the LoRA adaptation on or off during different phases of training or evaluation. For example, you might want to:\n","- **Enable LoRA during fine-tuning** to use the low-rank adaptation for updating the model.\n","- **Disable LoRA during evaluation** to measure the performance of the model without the low-rank adjustments.\n","\n","This approach adds flexibility to control whether the model uses the parameterized weights or the original weights based on the current requirement."],"metadata":{"id":"xinaP0u0p9xX"}},{"cell_type":"code","source":["total_parameters_lora = 0\n","total_parameters_non_lora = 0\n","for index, layer in enumerate([cnet.fc1, cnet.fc2, cnet.fc3]):\n","    total_parameters_lora += layer.parametrizations[\"weight\"][0].lora_A.nelement() + layer.parametrizations[\"weight\"][0].lora_B.nelement()\n","    total_parameters_non_lora += layer.weight.nelement() + layer.bias.nelement()\n","    print(\n","        f'Layer {index+1}: W: {layer.weight.shape} + B: {layer.bias.shape} + Lora_A: {layer.parametrizations[\"weight\"][0].lora_A.shape} + Lora_B: {layer.parametrizations[\"weight\"][0].lora_B.shape}'\n","    )\n","# The non-LoRA parameters count must match the original network\n","assert total_parameters_non_lora == total_parameters_original\n","print(f'Total number of parameters (original): {total_parameters_non_lora:,}')\n","print(f'Total number of parameters (original + LoRA): {total_parameters_lora + total_parameters_non_lora:,}')\n","print(f'Parameters introduced by LoRA: {total_parameters_lora:,}')\n","parameters_incremment = (total_parameters_lora / total_parameters_non_lora) * 100\n","print(f'Parameters incremment: {parameters_incremment:.3f}%')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFPYSohLGGsZ","executionInfo":{"status":"ok","timestamp":1729256780013,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"434f5977-ba66-4ffb-e368-64978822927d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Layer 1: W: torch.Size([1000, 784]) + B: torch.Size([1000]) + Lora_A: torch.Size([1, 784]) + Lora_B: torch.Size([1000, 1])\n","Layer 2: W: torch.Size([2000, 1000]) + B: torch.Size([2000]) + Lora_A: torch.Size([1, 1000]) + Lora_B: torch.Size([2000, 1])\n","Layer 3: W: torch.Size([10, 2000]) + B: torch.Size([10]) + Lora_A: torch.Size([1, 2000]) + Lora_B: torch.Size([10, 1])\n","Total number of parameters (original): 2,807,010\n","Total number of parameters (original + LoRA): 2,813,804\n","Parameters introduced by LoRA: 6,794\n","Parameters incremment: 0.242%\n"]}]},{"cell_type":"markdown","source":["Freeze all the parameters of the original network and only fine tuning the ones introduced by LoRA. Then fine-tune the model on the digit 9 and only for 100 batches."],"metadata":{"id":"UGJtlHNgd_1Y"}},{"cell_type":"code","source":["# Freeze the non-Lora parameters\n","for name, param in cnet.named_parameters():\n","    if 'lora' not in name:\n","        print(f'Freezing non-LoRA parameter {name}')\n","        param.requires_grad = False\n","\n","# Load the MNIST dataset again, by keeping only the digit 8\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","exclude_indices = mnist_trainset.targets == 8\n","mnist_trainset.data = mnist_trainset.data[exclude_indices]\n","mnist_trainset.targets = mnist_trainset.targets[exclude_indices]\n","# Create a dataloader for the training\n","train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n","\n","# Train the network with LoRA only on the digit 8 and only for 100 batches (hoping that it would improve the performance on the digit 8)\n","train(train_loader, cnet, epochs=1, total_iterations_limit=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EpVThrtPd-lg","executionInfo":{"status":"ok","timestamp":1729256780429,"user_tz":-330,"elapsed":427,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"b9c9b25d-8dbb-4f82-8dc0-4ebdb541361a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Freezing non-LoRA parameter fc1.bias\n","Freezing non-LoRA parameter fc1.parametrizations.weight.original\n","Freezing non-LoRA parameter fc2.bias\n","Freezing non-LoRA parameter fc2.parametrizations.weight.original\n","Freezing non-LoRA parameter fc3.bias\n","Freezing non-LoRA parameter fc3.parametrizations.weight.original\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1:  90%|█████████ | 9/10 [00:00<00:00, 22.10it/s, loss=0.0493]\n"]}]},{"cell_type":"markdown","source":["Verify that the fine-tuning didn't alter the original weights, but only the ones introduced by LoRA."],"metadata":{"id":"2aBchKZ8f3ZT"}},{"cell_type":"code","source":["# Check that the frozen parameters are still unchanged by the finetuning\n","assert torch.all(cnet.fc1.parametrizations.weight.original == original_weights['fc1.weight'])\n","assert torch.all(cnet.fc2.parametrizations.weight.original == original_weights['fc2.weight'])\n","assert torch.all(cnet.fc3.parametrizations.weight.original == original_weights['fc3.weight'])"],"metadata":{"id":"0NQ9Hgbgfref"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enable_disable_lora(enabled=True)\n","# The new fc1.weight is obtained by the \"forward\" function of our LoRA parametrization\n","# The original weights have been moved to net.fc1.parametrizations.weight.original\n","# More info here: https://pytorch.org/tutorials/intermediate/parametrizations.html#inspecting-a-parametrized-module\n","assert torch.equal(cnet.fc1.weight, cnet.fc1.parametrizations.weight.original + (cnet.fc1.parametrizations.weight[0].lora_B @ cnet.fc1.parametrizations.weight[0].lora_A) * cnet.fc1.parametrizations.weight[0].scale)\n","\n","enable_disable_lora(enabled=False)\n","# If we disable LoRA, the fc1.weight is the original one\n","assert torch.equal(cnet.fc1.weight, original_weights['fc1.weight'])"],"metadata":{"id":"ylPOjsgVgG2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test the network with LoRA enabled (the digit 9 should be classified better)"],"metadata":{"id":"L6d4MaJzr74X"}},{"cell_type":"code","source":["# Test with LoRA enabled\n","enable_disable_lora(enabled=True)\n","infer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m6Q_EahrsCMr","executionInfo":{"status":"ok","timestamp":1729256784361,"user_tz":-330,"elapsed":3934,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"a94485c8-cff2-4ed1-952a-bb357bb8eede"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.931\n","Wrong predictions for class 0: 19\n","Wrong predictions for class 1: 41\n","Wrong predictions for class 2: 54\n","Wrong predictions for class 3: 113\n","Wrong predictions for class 4: 23\n","Wrong predictions for class 5: 85\n","Wrong predictions for class 6: 36\n","Wrong predictions for class 7: 84\n","Wrong predictions for class 8: 16\n","Wrong predictions for class 9: 223\n"]}]},{"cell_type":"code","source":["# Test with LoRA disabled\n","enable_disable_lora(enabled=False)\n","infer()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VLjvAyGlsmPF","executionInfo":{"status":"ok","timestamp":1729256787778,"user_tz":-330,"elapsed":3420,"user":{"displayName":"Shakirali Vijapura","userId":"01693422934316313671"}},"outputId":"e2c847bd-d53b-4bcb-c3b7-a172d2cd8b16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.965\n","Wrong predictions for class 0: 17\n","Wrong predictions for class 1: 16\n","Wrong predictions for class 2: 30\n","Wrong predictions for class 3: 32\n","Wrong predictions for class 4: 15\n","Wrong predictions for class 5: 35\n","Wrong predictions for class 6: 29\n","Wrong predictions for class 7: 40\n","Wrong predictions for class 8: 39\n","Wrong predictions for class 9: 97\n"]}]}]}